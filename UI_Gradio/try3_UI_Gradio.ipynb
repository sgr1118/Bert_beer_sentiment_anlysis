{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio - ML작업 UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 유사 키워드 맵핑 결과 감정별 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gyeom\\OneDrive - 한국항공대학교\\바탕 화면\\머신러닝\\프로젝트\\맥주 측면 감정 분석\\Beer_Sentiment_analysis\\Data\n"
     ]
    }
   ],
   "source": [
    "cd \"./Beer_Sentiment_analysis/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px dashed gray;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 명사 위주 동의어 집계 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# 키워드의 초기 빈도 수를 계산\n",
    "def get_initial_counts(keywords):\n",
    "    initial_counts = defaultdict(int)\n",
    "    for keyword in keywords:\n",
    "        initial_counts[keyword] += 1\n",
    "    return initial_counts\n",
    "\n",
    "# 명사 토큰 저장하고 각 유사성을 계산하여 명사 유사성 임계값을 초과하는지 확인\n",
    "# 형용사의 차이는 개인차에 의해 발생하고 명사는 특징 자체를 나타내기에 집중 그룹핑 대상이 됨\n",
    "def custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "    doc1_nouns = [token for token in doc1 if token.pos_ == 'NOUN']\n",
    "    doc2_nouns = [token for token in doc2 if token.pos_ == 'NOUN']\n",
    "    noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n",
    "    return noun_similar\n",
    "\n",
    "# 유사한 키워드들을 병합하고 키워드 빈도 수와 매핑 저장\n",
    "def merge_keywords(keyword_counts, noun_similarity_threshold):\n",
    "    merged_keywords = defaultdict(int)\n",
    "    keyword_mappings = defaultdict(list)\n",
    "    keyword_docs = {}\n",
    "\n",
    "    # 이미 등록된 키워드는 nlp 계산(임베딩)하지 않고 바로 리턴 (Memoization)\n",
    "    def get_keyword_doc(keyword):\n",
    "        if keyword not in keyword_docs:\n",
    "            keyword_docs[keyword] = nlp(keyword)\n",
    "        return keyword_docs[keyword]\n",
    "\n",
    "    \n",
    "    # 초기 빈도 딕셔너리를 순회하며 각 키워드에 대해 임베딩\n",
    "    for keyword, count in keyword_counts.items():\n",
    "        doc1 = get_keyword_doc(keyword)\n",
    "        merged_or_found_similar = False\n",
    "\n",
    "        # 이미 병합된 키워드 목록에서 해당 키워드와 비교\n",
    "        for merged_keyword in list(merged_keywords):\n",
    "            # 이미 병합된 키워드에 현재 키워드가 포함되어 있다면 빈도를 더함\n",
    "            if keyword == merged_keyword:\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "            doc2 = get_keyword_doc(merged_keyword)\n",
    "            \n",
    "            # 유사도가 임계값보다 큰 경우, 두 키워드를 같은 그룹으로 묶음\n",
    "            if custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "        # 현재 키워드가 기존 병합된 키워드 목록에 포함되지 않고, 유사한 키워드도 없다면 새로운 항목으로 추가\n",
    "        if not merged_or_found_similar:\n",
    "            merged_keywords[keyword] = count\n",
    "            keyword_mappings[keyword] = [keyword]\n",
    "\n",
    "    return merged_keywords, keyword_mappings\n",
    "\n",
    "def merge_similar_keywords_noun_weight(dataframe, noun_similarity_threshold=0.7):\n",
    "    # 문자열로 표현된 리스트를 실제 리스트로 변환\n",
    "    dataframe['Keywords_List'] = dataframe['Keywords'].apply(lambda x: ast.literal_eval(x))\n",
    "    all_keywords = dataframe['Keywords_List'].sum()\n",
    "\n",
    "    # 빈도 수 계산 및 병합\n",
    "    initial_counts = get_initial_counts(all_keywords)\n",
    "    filtered_and_merged_keywords, merged_keyword_mappings = merge_keywords(initial_counts, noun_similarity_threshold)\n",
    "    sorted_merged_keywords = sorted(filtered_and_merged_keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 변환된 'Keywords_List' 컬럼 삭제\n",
    "    dataframe.drop('Keywords_List', axis=1, inplace=True)\n",
    "\n",
    "    return dict(sorted_merged_keywords), dict(merged_keyword_mappings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px dashed gray;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 시도2_집계+매핑 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://35a8b168144390afcc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://35a8b168144390afcc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gyeom\\AppData\\Local\\Temp\\ipykernel_14516\\2139005315.py:19: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n",
      "C:\\Users\\Gyeom\\AppData\\Local\\Temp\\ipykernel_14516\\2139005315.py:19: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from itertools import islice\n",
    "\n",
    "beer_df = pd.read_csv('pre_apply_beer_Wired_iStout3_JJ_NN.csv')\n",
    "\n",
    "def show_keywords(beer_name, sentiment, flag=0):\n",
    "    one_beer_df = beer_df[beer_df['Beer_name'] == beer_name]\n",
    "    df = one_beer_df[one_beer_df['MultinomialNB_label'] == sentiment]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    keywords, mappings = merge_similar_keywords_noun_weight(df)\n",
    "\n",
    "    if flag == 1:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)])\n",
    "    else:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)]), mappings\n",
    "\n",
    "def keyword_mappings(beer_name, sentiment, keyword):\n",
    "    _, mappings = show_keywords(beer_name, sentiment)\n",
    "    mapped_keywords = mappings.get(keyword, \"해당 키워드에 매핑된 문자열이 없습니다.\")\n",
    "    return mapped_keywords\n",
    "\n",
    "beer_names = list(beer_df['Beer_name'].unique())\n",
    "sentiments = [\"Positive\", \"Negative\"]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            beer_name_dropdown = gr.Dropdown(choices=beer_names, label=\"Beer_name\", info=\"Choose the beer you want to see the summary keyword.\")\n",
    "            sentiment_dropdown = gr.Dropdown(choices=sentiments, label=\"Sentiment\", info=\"Choose Positive or Negative sentiment.\")\n",
    "            keyword_input = gr.Textbox(label=\"Keyword\", info=\"Enter a keyword to see its mappings.\")\n",
    "        with gr.Column():\n",
    "            output_keywords = gr.Textbox(label=\"Summary keywords (Top 10)\")\n",
    "            output_mapping = gr.Textbox(label=\"Keyword mapping\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            review_button = gr.Button(\"Review Topic\")\n",
    "        with gr.Column():\n",
    "            mapping_button = gr.Button(\"Keyword mapping\")\n",
    "\n",
    "    review_button.click(fn=lambda beer_name, sentiment: show_keywords(beer_name, sentiment, flag=1),\n",
    "                    inputs=[beer_name_dropdown, sentiment_dropdown],\n",
    "                    outputs=output_keywords)\n",
    "    mapping_button.click(keyword_mappings,\n",
    "                        inputs=[beer_name_dropdown, sentiment_dropdown, keyword_input],\n",
    "                        outputs=output_mapping)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://416ba780330f1270fd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://416ba780330f1270fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import pandas as pd\n",
    "import gradio as gra\n",
    "import gradio as gr\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "# 모델 로드\n",
    "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/sentiment_bert/sentiment_model_BERT_Attention_v2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def analyze_sentiment(sentence):\n",
    "    # 문장을 토크나이징하고 모델 입력으로 변환\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # 모델을 통해 감정 분류 수행\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = softmax(logits, dim=1)\n",
    "\n",
    "    # 감정 분류 확률 추출\n",
    "    sentiment_labels = ['Negative', 'Positive']\n",
    "    sentiment_probabilities = {label: probability.item() for label, probability in zip(sentiment_labels, probabilities[0])}\n",
    "\n",
    "    return sentiment_probabilities\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    sentiment_probabilities = analyze_sentiment(text)\n",
    "    return sentiment_probabilities\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# 키워드의 초기 빈도 수를 계산\n",
    "def get_initial_counts(keywords):\n",
    "    initial_counts = defaultdict(int)\n",
    "    for keyword in keywords:\n",
    "        initial_counts[keyword] += 1\n",
    "    return initial_counts\n",
    "\n",
    "# 명사 토큰 저장하고 각 유사성을 계산하여 명사 유사성 임계값을 초과하는지 확인\n",
    "# 형용사의 차이는 개인차에 의해 발생하고 명사는 특징 자체를 나타내기에 집중 그룹핑 대상이 됨\n",
    "def custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "    doc1_nouns = [token for token in doc1 if token.pos_ == 'NOUN']\n",
    "    doc2_nouns = [token for token in doc2 if token.pos_ == 'NOUN']\n",
    "    noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n",
    "    return noun_similar\n",
    "\n",
    "# 유사한 키워드들을 병합하고 키워드 빈도 수와 매핑 저장\n",
    "def merge_keywords(keyword_counts, noun_similarity_threshold):\n",
    "    merged_keywords = defaultdict(int)\n",
    "    keyword_mappings = defaultdict(list)\n",
    "    keyword_docs = {}\n",
    "\n",
    "    # 이미 등록된 키워드는 nlp 계산(임베딩)하지 않고 바로 리턴 (Memoization)\n",
    "    def get_keyword_doc(keyword):\n",
    "        if keyword not in keyword_docs:\n",
    "            keyword_docs[keyword] = nlp(keyword)\n",
    "        return keyword_docs[keyword]\n",
    "\n",
    "\n",
    "    # 초기 빈도 딕셔너리를 순회하며 각 키워드에 대해 임베딩\n",
    "    for keyword, count in keyword_counts.items():\n",
    "        doc1 = get_keyword_doc(keyword)\n",
    "        merged_or_found_similar = False\n",
    "\n",
    "        # 이미 병합된 키워드 목록에서 해당 키워드와 비교\n",
    "        for merged_keyword in list(merged_keywords):\n",
    "            # 이미 병합된 키워드에 현재 키워드가 포함되어 있다면 빈도를 더함\n",
    "            if keyword == merged_keyword:\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "            doc2 = get_keyword_doc(merged_keyword)\n",
    "\n",
    "            # 유사도가 임계값보다 큰 경우, 두 키워드를 같은 그룹으로 묶음\n",
    "            if custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "        # 현재 키워드가 기존 병합된 키워드 목록에 포함되지 않고, 유사한 키워드도 없다면 새로운 항목으로 추가\n",
    "        if not merged_or_found_similar:\n",
    "            merged_keywords[keyword] = count\n",
    "            keyword_mappings[keyword] = [keyword]\n",
    "\n",
    "    return merged_keywords, keyword_mappings\n",
    "\n",
    "def merge_similar_keywords_noun_weight(dataframe, noun_similarity_threshold=0.7):\n",
    "    # 문자열로 표현된 리스트를 실제 리스트로 변환\n",
    "    dataframe['Keywords_List'] = dataframe['Keywords'].apply(lambda x: ast.literal_eval(x))\n",
    "    all_keywords = dataframe['Keywords_List'].sum()\n",
    "\n",
    "    # 빈도 수 계산 및 병합\n",
    "    initial_counts = get_initial_counts(all_keywords)\n",
    "    filtered_and_merged_keywords, merged_keyword_mappings = merge_keywords(initial_counts, noun_similarity_threshold)\n",
    "    sorted_merged_keywords = sorted(filtered_and_merged_keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 변환된 'Keywords_List' 컬럼 삭제\n",
    "    dataframe.drop('Keywords_List', axis=1, inplace=True)\n",
    "\n",
    "    return dict(sorted_merged_keywords), dict(merged_keyword_mappings)\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "beer_df = pd.read_csv('pre_apply_beer_Wired_iStout3_JJ_NN.csv')\n",
    "\n",
    "def show_keywords(beer_name, sentiment, flag=0):\n",
    "    one_beer_df = beer_df[beer_df['Beer_name'] == beer_name]\n",
    "    df = one_beer_df[one_beer_df['MultinomialNB_label'] == sentiment]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    keywords, mappings = merge_similar_keywords_noun_weight(df)\n",
    "\n",
    "    if flag == 1:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)])\n",
    "    else:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)]), mappings\n",
    "\n",
    "def keyword_mappings(beer_name, sentiment, keyword):\n",
    "    _, mappings = show_keywords(beer_name, sentiment)\n",
    "    mapped_keywords = mappings.get(keyword, \"해당 키워드에 매핑된 문자열이 없습니다.\")\n",
    "    return mapped_keywords\n",
    "\n",
    "beer_names = beer_df['Beer_name'].unique()\n",
    "sentiments = [\"Positive\", \"Negative\"]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            beer_name_dropdown = gr.Dropdown(choices=beer_names, label=\"Beer_name\", info=\"Choose the beer you want to see the summary keyword.\")\n",
    "            sentiment_dropdown = gr.Dropdown(choices=sentiments, label=\"Sentiment\", info=\"Choose Positive or Negative sentiment.\")\n",
    "            keyword_input = gr.Textbox(label=\"Keyword\", info=\"Enter a keyword to see its mappings.\")\n",
    "        with gr.Column():\n",
    "            output_keywords = gr.Textbox(label=\"Summary keywords (Top 10)\")\n",
    "            output_mapping = gr.Textbox(label=\"Keyword mapping\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            review_button = gr.Button(\"Review Topic\")\n",
    "        with gr.Column():\n",
    "            mapping_button = gr.Button(\"Keyword mapping\")\n",
    "\n",
    "    review_button.click(fn=lambda beer_name, sentiment: show_keywords(beer_name, sentiment, flag=1),\n",
    "                    inputs=[beer_name_dropdown, sentiment_dropdown],\n",
    "                    outputs=output_keywords)\n",
    "    mapping_button.click(keyword_mappings,\n",
    "                        inputs=[beer_name_dropdown, sentiment_dropdown, keyword_input],\n",
    "                        outputs=output_mapping)\n",
    "\n",
    "# Tab 1\n",
    "app1 = gra.Interface(fn=sentiment_analysis,\n",
    "                     inputs=\"text\",\n",
    "                     outputs=\"label\",\n",
    "                     live=True,\n",
    "                     title = \"Beer Sentiment Analysis\")\n",
    "\n",
    "# Tab 2\n",
    "app2 = demo\n",
    "\n",
    "# 탭 1과 2를 그룹화\n",
    "tabbed = gra.TabbedInterface([app1,app2],\n",
    "                             [ 'Sentiment Analysis' , 'Keyword Extraction'])\n",
    "tabbed.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ui_python",
   "language": "python",
   "name": "ui_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
